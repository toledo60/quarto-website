{
  "hash": "1171ac1b6706ff9025bfad88a671cceb",
  "result": {
    "markdown": "---\ntoc: true\n---\n\n\n\n\n<!---\nIn this lab, you will\n\n1. Understand linear regression in R and verify linear regression assumptions\n2. Plotting time series to analyse trends\n3. Use R for sampling and simulation\n\nThroughout this tutorial we will utilize the `penguins` dataset found in `palmerpenguins` package\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages('palmerpenguins')\n```\n:::\n\nLoading the library will now give us access to the penguins dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\n```\n:::\n\n\nsee `?penguins`  or @palmer, for more info on this dataset\n\n\n## Simple Linear Regression\n\nSimple linear regression is a statistical method which allows us to predict a quantitative outcome $y$ on the basis of a single predictor variable $x$. \n\n* Sometimes $x$ is regarded as the *predictor, explanatory,* or *independent variable*.\n\n* Similarly, $y$ is regarded as the *response, outcome,* or *dependent variable*\n\nWe will only use *predictor* and *response* to denote our variables of interest. The goal is to define a statistical model that defines the response variable (*y*) as a function of the predictor (*x*) variable.\nOnce, we built a statistically significant model, itâ€™s possible to use it for predicting outcomes on the basis of new predictor values.\n\nBelow are all the numerical variables in our dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(Filter(is.numeric,penguins))\n```\n:::\n\n\nFor our simple linear regression model, we will consider the body mass *(grams)* of the penguins as the response variable and the length of the length of the the penguin's flipper *(millimeters)* as our single predictor variable\n\nOur model takes the form \n$$\n(body_{mass}) = \\beta_0 + \\beta_1 \\cdot (flipper_{length}) + \\epsilon\n$$\nwhere \n\n* $\\beta_0$ is the intercept of the regression line, (when flipper_length_mm =0)\n* $\\beta_1$ is the slope of the regression line\n* $\\epsilon$ is the error terms\n\n\nFor simplicity we will remove any missing data from our variables of interest\n\n::: {.cell}\n\n```{.r .cell-code}\nna_flipper  <- is.na(penguins$flipper_length_mm)\nna_body_mass <- is.na(penguins$body_mass_g)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwhich(na_flipper)\nwhich(na_body_mass)\n```\n:::\n\n\nwe will remove rows 4 and 272 from both body mass and flipper length since they have missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nflipper_length_mm <- penguins$flipper_length_mm[!na_flipper]\nbody_mass_g <- penguins$body_mass_g[!na_body_mass]\n```\n:::\n\n\nIn most cases missing data can occur in different rows for various variables, so it not recommended to remove missing data like we did above. Since we are only considering flipper length and body mass for our linear model, and both share missing data from the same row entries we can remove them as we did above.\n\nMore examples on removing missing data can be found on this [sparkbyexamples](https://sparkbyexamples.com/r-programming/remove-rows-with-na-in-r/) tutorial\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(flipper_length_mm,body_mass_g,\n     xlab = 'Flipper length (mm)',\n     ylab = 'Body mass (grams)')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_lm <- lm(body_mass_g ~ flipper_length_mm)\nsimple_lm\n```\n:::\n\n\nThe model can be written as `body_mass_g = -5780.83 + 49.69*flipper_length_mm`, this is our line of best fit. We can then plot our line of best fit as shown below\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(flipper_length_mm,body_mass_g,\n     xlab = 'Flipper length (mm)',\n     ylab = 'Body mass (grams)')\nabline(simple_lm,col = 'red',lwd =2)\n```\n:::\n\n\n\nMoreover, we can use the `summary()` function to quickly check whether our predictor is significantly associated with the response variable. If so, we can further assess how well our model fits our actual data by utilizing metrics provided by the summary output\n\n\n### Model Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_summary <- summary(simple_lm)\nlm_summary\n```\n:::\n\n\n\nThe '*' symbols indicate the level of significance for the respective term. The significance codes below the line shows the definitions for the level of significance; one star means $0.01 < p < 0.05$, two stars mean $0.001 < p < 0.01$, and similar interpretations for the remaining symbols\n\n\nBreaking down the `summary` output, it contains the following components *(Extract using `$` operator)*\n\n### Call\nThe function (formula) used to compute the regression model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_summary$call\n```\n:::\n\n\n### Residuals\n\nThe difference between predicted values for the response variable using our constructed model and the observed response values from our original data. Mathematically, \n$$\nr_i = y_i - \\hat{y}_i\n$$\n\nIf the residual is \n\n* *Positive:* The predicted value was an underestimate of the observed response\n* *Negative:* The predicted value was an overestimate of the observed response\n* *Zero:* The predicted value is exactly the same as the observed response\n\n\n<details><summary>Show Code</summary>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- cbind.data.frame(flipper_length_mm,body_mass_g)\n\np1 <- ggplot(dat, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_smooth(method = \"lm\", se = FALSE, \n              color = \"lightgrey\",formula = 'y~x') +\n  geom_segment(aes(xend = flipper_length_mm, \n                   yend = predict(simple_lm)), alpha = .2) +\n  geom_point(aes(color=residuals(simple_lm))) +\n  guides(color = 'none') + \n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  labs(x = 'Flipper length (mm)',\n       y = 'Body mass (grams)')+\n  theme_bw()+\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank())\n```\n:::\n\n</details>\n\n\n\n::: {.cell}\n\n:::\n\n\n\nThe figure above shows the magnitude of the resulting residuals from our fitted model. The darker the red points the more positive the residuals were, the darker the blue points the more negative the residuals were.\n\nAssuming our line is the \"line of best fit\" the sum of the residuals always equals zero\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(lm_summary$residuals)\n```\n:::\n\n\nA few plots of the residuals to assess our regression assumptions:\n\nResiduals vs fitted values plot is used to detect non-linearity, unequal error variances, and possible outliers\n\nThe residuals should form a \"horizontal band\" around the y=0 line, suggesting the assumption that the relationship is linear is reasonable as well that the variances of the error terms are equal\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(body_mass_g,simple_lm$residuals,\n     xlab = 'Fitted values',\n     ylab = 'Residuals',\n     main = 'Residuals vs Fitted Values')\nabline(a = 0, b = 0, col = 'red', lwd = 2)\n```\n:::\n\n\n\nThe following histogram of residuals suggests that the residuals (and hence the error terms) are normally distributed\n\n<details><summary>Show Code</summary>\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_residuals <- simple_lm$residuals\nh <- hist(lm_residuals,\n          main = 'Distribution of Residuals',\n          xlab = 'Residuals')\nxfit <- seq(min(lm_residuals), max(lm_residuals), length = 40) \nyfit <- dnorm(xfit, mean = mean(lm_residuals), sd = sd(lm_residuals))\nyfit <- yfit * diff(h$mids[1:2]) * length(lm_residuals) \nlines(xfit, yfit, col = \"red\", lwd = 2)\n```\n:::\n\n</details>\n\n\n::: {.cell}\n\n:::\n\n\n\n### Coefficients\n\nEstimated regression beta coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1)$, alongside their standard errors, $t$-test, and $p$-values. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_summary$coefficients\n```\n:::\n\n\nFrom the output above:\n\n* the model can be written as `body_mass_g = -5780.831 + 49.686*flipper_length_mm`\n\n* the intercept $\\beta_0$ is -5780.831. It can be interpreted as the predicted body mass in grams for a flipper length of zero millimeters. In most scenarios the interpretation of the intercept coefficient will not make sense. For example, when the length of the flipper is zero millimeters we predict the weight of the penguin will be about -5780 grams\n\n\n* the regression beta coefficient for the variable `flipper_length_mm` $\\beta_1  is 49.686. That is, for each additional millimeter of the flipper we expect the penguin to gain 49.686 grams of body mass. For example, if the length of the flipper is 200 millimeters then we expect the penguin's body mass to be about `-5780.831 + 49.686 *200 = 4156.369` grams\n\n\n### Model Performance\n\n*Residual standard error (RSE), R-squared, Adjusted R-squared*, and the *F-statistic* are metrics used to check our model performance. That is, how well does our model fit the data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <- c(lm_summary$sigma,lm_summary$r.squared,\n             lm_summary$adj.r.squared,\n             lm_summary$fstatistic['value'])\n\nnames(metrics) <- c('RSE','R_squared','Adj_R_squared','F_statistic')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics\n```\n:::\n\n\n#### Residual standard error\n\nThe average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors\nThe closer to this value is to zero the better.\n\n#### R-squared/Adjusted R-squared\n\nThe proportion of information *(i.e. variation)* in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom. The higher these values are the better.\n\nIn the simple linear regression setting *R-squared* is the square of the Pearson correlation coefficient \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(flipper_length_mm,body_mass_g)^2\n```\n:::\n\n\n\n####  F-statistic \n\nThe *F-statistic* gives the overall significance of the model. It assess whether at least one predictor variable has a non-zero coefficient. The higher this value the better. However,this test only becomes more important when dealing with multiple predictors instead of a single predictor found in a simple linear regression.\n\nA large *F-statistic* will corresponds to a statistically significant *p-value* $(p < 0.05)$. In our example, the *F-statistic* equal 1070.74 producing a *p-value* less than  $2.2e-16$, which is highly significant\n\n\n--->\n\n",
    "supporting": [
      "lab3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}