[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "UCLA STATS 10: Introduction to Statistical Reasoning (Winter 2023)"
  },
  {
    "objectID": "teaching.html#previous-courses",
    "href": "teaching.html#previous-courses",
    "title": "Jose Toledo Luna",
    "section": "Previous Courses",
    "text": "Previous Courses\n\nUniversity of California, Los Angeles\n\nSTATS 10: Introduction to Statistical Reasoning (Fall 2022)\nSTATS 13: Introduction to Statistical Methods for Life\nand Health Sciences (Summer 2022)\n\n\n\nCalifornia State University, Fullerton\n\nMath 115: College Algebra (Fall 2019)\nMath 120: Elementary Statistics (Spring 2020, Fall 2020)\nMath 338: Statistics Applied to Natural Sciences (Spring 2021)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "Luna, J. , Jaynes, J. , Xu, H. , Wong, WK. Orthogonal array composite designs for drug combination experiments with applications for tuberculosis. Statistics in Medicine. 2022; 1- 18"
  },
  {
    "objectID": "research.html#software",
    "href": "research.html#software",
    "title": "Jose Toledo Luna",
    "section": "Software",
    "text": "Software\n\nggDoE: Create commonly used graphs in Design of Experiments with ggplot2\npyLHD: Latin Hypercube Designs for Python"
  },
  {
    "objectID": "zoom.html",
    "href": "zoom.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "Week\nDate\nTopic\nLink\n\n\n\n\nWeek 1\nMon, Jan 09\nLab 1: Installing R and R studio\nLecture 1\n\n\n\nWed, Jan 11\nLab 1: Variables, vectors\nLecture 2\n\n\nWeek 2\nMon, Jan 16\nHOLIDAY\nR markdown tutorial\n\n\n\nWed, Jan 18\nLab 1: Importing data, summarize/visualize single numerical variable\nLecture 3\n\n\nWeek 3\nMon, Jan 23\nLab1: Summarize Two numerical variables, One categorical variable\nLecture 4\n\n\n\nWed, Jan 25"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "I am currently a second year Statistics Ph.D. student at University of California, Los Angeles (UCLA) working with Hongquan Xu. Prior to joining UCLA, I obtained my BA and MS degrees in Statistics from California State University, Fullerton. You can learn more about me from my CV\nI am gratefully funded by the Eugene V. Cota-Robles Fellowship\nEmail: toledo60@g.ucla.edu"
  },
  {
    "objectID": "ucla/stats10/intro.html",
    "href": "ucla/stats10/intro.html",
    "title": "Welcome To STATS10",
    "section": "",
    "text": "Useful Links\n\n\n\n\nZoom For online discussion and/or virtual office hours\nBox Folder All material for the labs will be posted here\ncampuswire Have discussions or ask questions regarding this course\nLecture Recordings Recordings of lectures and other tutorials\n\n\n\nDiscussions:  4A M/W: 12:00pm-12:50pm (Online)  4B M/W: 1:00pm-1:50pm (PAB 1749)\nOffice Hours:  M/W: 12:00pm-12:50pm  @MS8349 or Zoom  or by appointment (Email me)\nEmail: toledo60@g.ucla.edu\nMaterial from this course can be downloaded from this Box Folder\nFor general questions or specific questions about the labs use campuswire. Otherwise, you can email me personally. The class code to join campuswire is posted on CANVAS\nAll lab assignments will be turned in directly on CANVAS\nBelow are all the labs regrading this course. In labs, you will apply the concepts discussed in lectures but with a focus on computation. All labs will be completed using R and documented with R markdown.\n\n\n\n\n\n\nWarning\n\n\n\nLab tutorials are a work in progress and are constantly being updated\n\n\n\nLab 1: Introduction to R\nLab 2: Logical Statements with Applications"
  },
  {
    "objectID": "ucla/stats10/lab1.html",
    "href": "ucla/stats10/lab1.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "This lab consists of learning fundamental concepts in R\n\n\nA variable provides us with named objects that our programs can manipulate. A valid variable name consists of letters, numbers and the dot or underline characters. It is important to note variable names are case sensitive. That is, var1 and Var1 are different variables. Below are appropriate variable names in R\n\n\n\n\n\n\n\nValid Variable Name\nReason\n\n\n\n\nvariable_name\nContains letters and underscore\n\n\nlong.variable_name\nContains letters, dot, and underscore\n\n\nvar\nContains letters\n\n\nvar1\nContains letters and numbers\n\n\nlong.variable_name2\nContains letters, numbers, dot and underscore\n\n\nvar1_name.1\nContains letters, numbers, dot and underscore\n\n\n.var_name\nCan start with period, contains letters and underscore\n\n\n\nThis is a good starting point for valid variable names. Next we demonstrate a few examples where variable names are invalid.\n\n\n\n\n\n\n\nInvalid Variable Names\nReason\n\n\n\n\n2var\nStarts with a number\n\n\n_varname\nStarts with underscore\n\n\n.2var_name\nWhile starting with a (.) dot is valid, it can not be followed by a number\n\n\n\nNow that we have an idea of how to name variables, lets discuss variable assignments. Variables can be assigned values using leftward (<-), rightward (->) and equal (=) operators. However, we will only stick with the leftward and equal assignment operators.\n\nvar_name1 <- 10\nvar2 = 20\nvar.name3 <- 30\nvar_name_4 = 40\n\n\n\n\nThe easiest method to create any type of vector in R is using c() (as in concatenate). We primarily focus on two types of vectors; numeric and character\n\n\n\nnum_vec <- c(0,1,2,3,4)\ntypeof(num_vec)\n\n#> [1] \"double\"\n\nclass(num_vec)\n\n#> [1] \"numeric\"\n\n\nUsing c() is not the only way to generate a vector, we can also generate the above vector using seq() as follows\n\nseq(from=0,to=4)\n\n#> [1] 0 1 2 3 4\n\n\nAnother approach to generate the same sequence can be done using 0:4\n\n0:4\n\n#> [1] 0 1 2 3 4\n\n\nor more generally a:b, where a is the starting number and b is the last number in the sequence\nWe can apply arithmetic operations to our numerical vector num_vec, such as addition, subtraction, multiplication, division, and exponentiation. These operations will be applied to each element in the vector (element-wise).\n\n\n\nOperator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n^\nExponent\n\n\n%%\nModulus (Remainder from division)\n\n\n%/%\nInteger Division\n\n\n\nArithmetic operations applied to numeric vectors follow PEMDAS order of operations, demonstrated in the following example\nSubtract 1 from each element\n\n(num_vec-1)\n\n#> [1] -1  0  1  2  3\n\n\nSubtract 1 from each element, then square them\n\n(num_vec-1)^2\n\n#> [1] 1 0 1 4 9\n\n\nSubtract 1 from each element, square them, then double each element\n\n 2*(num_vec - 1)^2\n\n#> [1]  2  0  2  8 18\n\n\nSubtract 1 from each element, square them, double them, then add 1 to each element\n\n2*(num_vec - 1)^2 + 1\n\n#> [1]  3  1  3  9 19\n\n\n\npemdas_vec <- 2*(num_vec - 1)^2 + 1\npemdas_vec\n\n#> [1]  3  1  3  9 19\n\n\nGenerating an odd sequence from 1 to 9, we can use c() or seq()\n\nc(1,3,5,7,9)\n\n#> [1] 1 3 5 7 9\n\nseq(from =1,to=10,by=2)\n\n#> [1] 1 3 5 7 9\n\n\nNote if you know the ordering of the arguments of a function it is not necessary to specify them. For example, it is optional to write from and to arguments in the seq() function\n\nseq(from = 1,to = 10)\n\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(1,10)\n\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\nchr_vec <- c('A','B',\"C\")\ntypeof(chr_vec)\n\n#> [1] \"character\"\n\nclass(chr_vec)\n\n#> [1] \"character\"\n\n\n\n\n\n\nThere are multiple ways to access or replace values in vectors. The most common approach is through “indexing”. It is important to know in starts with index 1.\n\nbig_vec <- 1:100\nbig_vec[1]\n\n#> [1] 1\n\nbig_vec[10] # extract the 10th element in your vector\n\n#> [1] 10\n\n\nFor accessing elements in a vector we can think vector[indices you want to extract] the way we extract certain elements can be through some condition, that is vector[condtion]\n\nbig_vec[ c(1,5,10) ]\n\n#> [1]  1  5 10\n\nbig_vec[ 1:10 ] # what are the first 10 elements ?\n\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n\n\nUsing c() we can concatenate elements from one vector into another vector. For example, we can add the elements from pemdas_vec into the existing vector num_vec\n\nc(num_vec,pemdas_vec)\n\n#>  [1]  0  1  2  3  4  3  1  3  9 19\n\n\nAlternatively, we can add the elements from num_vec into the existing vector pemdas_vec\n\nc(pemdas_vec,num_vec)\n\n#>  [1]  3  1  3  9 19  0  1  2  3  4\n\n\nYou will notice the order in which we concatenate the elements from the vectors does matter\n\nchr_vec\n\n#> [1] \"A\" \"B\" \"C\"\n\nchr_vec[1] <- 'a'\nchr_vec\n\n#> [1] \"a\" \"B\" \"C\"\n\n\n\nnum_vec\n\n#> [1] 0 1 2 3 4\n\nnum_vec[3] <- 10\nnum_vec\n\n#> [1]  0  1 10  3  4\n\nnum_vec[ c(1,3) ] <- c(100,200)\nnum_vec\n\n#> [1] 100   1 200   3   4\n\nnum_vec[c(1,2,3)] <- 0\nnum_vec\n\n#> [1] 0 0 0 3 4\n\n\n\n\n\nWhile base R contains a wide collection of useful functions and datasets, it might be necessary to install additional R packages to increase the power of R by improving existing base R functionalities, or by adding new ones.\nIn general, you can use this template to install a package in R:\n\ninstall.packages('package_name')\n\nFor example, in this lab we will need functions/datasets from the following package: maps. To install we simply type in our console\n\ninstall.packages('maps')\n\nAfter running the above command you should get something similar to the output below. The messages appeared will depend on what operating system you are using, the dependencies, and if the package was successfully installed.\n\ntrying URL 'https://cran.rstudio.com/bin/macosx/contrib/4.2/maps_3.4.0.tgz'\nContent type 'application/x-gzip' length 3105764 bytes (3.0 MB)\n==================================================\ndownloaded 3.0 MB\n\n\nThe downloaded binary packages are in\n    /var/folders/mc/rznpg9ks30sd6wdh7rchs4v40000gn/T//RtmpLUHvkq/downloaded_packages\n\nOnce the package was installed successfully we now have access to all of its functionalities/datasets. To access them we load the package into memory using the command library()\n\nlibrary(maps)\n\nHowever, if we only need to access say a specific function/dataset a few times we can do so using the notation packagename::functionname(). For example, if we only need to access the Canada cities data set in the maps package we run the following command\n\nmaps::canada.cities\n\n\n\n#>            name country.etc    pop   lat    long capital\n#> 1 Abbotsford BC          BC 157795 49.06 -122.30       0\n#> 2      Acton ON          ON   8308 43.63  -80.03       0\n#> 3 Acton Vale QC          QC   5153 45.63  -72.57       0\n#> 4    Airdrie AB          AB  25863 51.30 -114.02       0\n#> 5    Aklavik NT          NT    643 68.22 -135.00       0\n\n\nAlternatively, if you loaded the entire package using library(maps) we can access the Canada cities data set using the following command\n\ncanada.cities\n\n\n\n#>            name country.etc    pop   lat    long capital\n#> 1 Abbotsford BC          BC 157795 49.06 -122.30       0\n#> 2      Acton ON          ON   8308 43.63  -80.03       0\n#> 3 Acton Vale QC          QC   5153 45.63  -72.57       0\n#> 4    Airdrie AB          AB  25863 51.30 -114.02       0\n#> 5    Aklavik NT          NT    643 68.22 -135.00       0"
  },
  {
    "objectID": "ucla/stats10/lab1.html#overview-of-births-dataset",
    "href": "ucla/stats10/lab1.html#overview-of-births-dataset",
    "title": "Jose Toledo Luna",
    "section": "Overview of Births Dataset",
    "text": "Overview of Births Dataset\nThe births dataset is a sample of information about babies born in North Carolina. It is considered a data frame because it contains numeric information about each baby, as well as various pieces of categorical data such as the race of the parents, and whether the parents had a smoking habit.\n\nbirth_dat <- read.csv(file=\"/Users/toledo60/Desktop/Projects/personal-website/ucla/stats10/data/births.csv\")\n\n\nbirth_dat\n\n\n\n#>   Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n#> 1   Male     No    124      8   31   25    13    14       1     13   Married\n#> 2 Female     No    177      8   36   26     9    12       2     11 Unmarried\n#> 3   Male     No    107      3   30   16    12     8       2     10 Unmarried\n#> 4 Female     No    144      6   33   37    12    14       2     12 Unmarried\n#> 5   Male     No    117      9   36   33    10    16       2     19   Married\n#>   Racemom Racedad Hispmom Hispdad Gained     Habit MomPriorCond BirthDef\n#> 1   White   White NotHisp NotHisp     40 NonSmoker         None     None\n#> 2   White   White Mexican Mexican     20 NonSmoker         None     None\n#> 3   White Unknown Mexican Unknown     70 NonSmoker At Least One     None\n#> 4   White   White NotHisp NotHisp     50 NonSmoker         None     None\n#> 5   White   Black NotHisp NotHisp     40 NonSmoker At Least One     None\n#>      DelivComp BirthComp\n#> 1 At Least One      None\n#> 2 At Least One      None\n#> 3 At Least One      None\n#> 4 At Least One      None\n#> 5         None      None\n\n\nWe can view the structure of our dataset using str() function\n\nstr(birth_dat) \n\n#> 'data.frame':    1992 obs. of  21 variables:\n#>  $ Gender      : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n#>  $ Premie      : chr  \"No\" \"No\" \"No\" \"No\" ...\n#>  $ weight      : int  124 177 107 144 117 98 147 138 104 123 ...\n#>  $ Apgar1      : int  8 8 3 6 9 4 8 9 9 9 ...\n#>  $ Fage        : int  31 36 30 33 36 31 33 22 30 23 ...\n#>  $ Mage        : int  25 26 16 37 33 29 30 20 21 18 ...\n#>  $ Feduc       : int  13 9 12 12 10 14 12 14 12 12 ...\n#>  $ Meduc       : int  14 12 8 14 16 16 9 14 12 12 ...\n#>  $ TotPreg     : int  1 2 2 2 2 3 4 1 1 1 ...\n#>  $ Visits      : int  13 11 10 12 19 20 16 10 30 16 ...\n#>  $ Marital     : chr  \"Married\" \"Unmarried\" \"Unmarried\" \"Unmarried\" ...\n#>  $ Racemom     : chr  \"White\" \"White\" \"White\" \"White\" ...\n#>  $ Racedad     : chr  \"White\" \"White\" \"Unknown\" \"White\" ...\n#>  $ Hispmom     : chr  \"NotHisp\" \"Mexican\" \"Mexican\" \"NotHisp\" ...\n#>  $ Hispdad     : chr  \"NotHisp\" \"Mexican\" \"Unknown\" \"NotHisp\" ...\n#>  $ Gained      : int  40 20 70 50 40 21 22 20 11 19 ...\n#>  $ Habit       : chr  \"NonSmoker\" \"NonSmoker\" \"NonSmoker\" \"NonSmoker\" ...\n#>  $ MomPriorCond: chr  \"None\" \"None\" \"At Least One\" \"None\" ...\n#>  $ BirthDef    : chr  \"None\" \"None\" \"None\" \"None\" ...\n#>  $ DelivComp   : chr  \"At Least One\" \"At Least One\" \"At Least One\" \"At Least One\" ...\n#>  $ BirthComp   : chr  \"None\" \"None\" \"None\" \"None\" ...\n\n\nThere are 1992 rows and 21 columns, and we can see the datatypes for each variable (column). Those with int are considered numerical variables and those with chr are considered categorical variables.\nNote: While looking at the structure of the data using str() we primarily saw two types of variables: chr and int to denote character and integer data types, respectively. However we can also have factor and double.\nfactor will be considered as a categorical variable and double would be considered as a numerical variable.\nWe can use the $ operator to access values from a data frame column. For example, if we want to extract the values from the Gender column from our dataset, we run the following command\n\nbirth_dat$Gender\n\n\n\n#> [1] \"Male\"   \"Female\" \"Male\"   \"Female\" \"Male\"   \"Female\" \"Male\"   \"Male\"\n\n\nIf we save these values into a variable we will be able to manipulate them without altering the dataset itself.\n\ngenders <- birth_dat$Gender"
  },
  {
    "objectID": "ucla/stats10/lab1.html#single-variable",
    "href": "ucla/stats10/lab1.html#single-variable",
    "title": "Jose Toledo Luna",
    "section": "Single variable",
    "text": "Single variable\n\nNumerical variables\nOne way to extract all the numerical columns is using both Filter and is.numeric functions. Below are the first five rows of all the numerical columns in birth_dat\n\nFilter(is.numeric,birth_dat)\n\n\n\n#>   weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits Gained\n#> 1    124      8   31   25    13    14       1     13     40\n#> 2    177      8   36   26     9    12       2     11     20\n#> 3    107      3   30   16    12     8       2     10     70\n#> 4    144      6   33   37    12    14       2     12     50\n#> 5    117      9   36   33    10    16       2     19     40\n\n\nThe names of the numerical columns can be obtained using colnames() function in combination with the above statement\n\ncolnames(Filter(is.numeric,birth_dat) )\n\n#> [1] \"weight\"  \"Apgar1\"  \"Fage\"    \"Mage\"    \"Feduc\"   \"Meduc\"   \"TotPreg\"\n#> [8] \"Visits\"  \"Gained\"\n\n\nWe will only consider the weight variable from our dataset to demonstrate methods to summarize and visualize a numerical variable.\nFunctions for numerical summaries include, but not limited to,\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nmean()\nmean\n\n\nmedian()\nmedian\n\n\nmode()\nmode\n\n\nsd()\nstandard deviation\n\n\nvar()\nvariance\n\n\nmin()\nminimum\n\n\nmax()\nmaximum\n\n\nsummary()\nComputes the following: Minimum ,1st Quartile, Median,Mean ,3rd Quartile,Maximum\n\n\n\nNext, we’ll save the values from weight column into a separate variable and compute several numerical summaries listed above\n\nbirth_weight <- birth_dat$weight\n\n\nmean(birth_weight)\n\n#> [1] 116.0512\n\nmedian(birth_weight)\n\n#> [1] 117\n\nmin(birth_weight)\n\n#> [1] 14\n\nmax(birth_weight)\n\n#> [1] 177\n\n\n\nsummary(birth_weight)\n\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>    14.0   106.0   117.0   116.1   129.0   177.0\n\n\nWhile summary() give us a quick numerical summary of our distribution, it is important to also visualize the overall distribution using a plot such as a boxplot\n\nboxplot(birth_weight)\n\n\n\n\nor a histogram\n\nhist(birth_weight)\n\n\n\n\nFrom the histogram above, while the overall distribution of the birth weights is symmetrical there are outliers causing the distribution to be skewed to the left.\nFor boxplot() and hist() we used the default settings, while they are informative we can alter their appearance to be more professional.\nFor example, we changed the x-axis label and y-axis label using xlab,ylab arguments,respectively. We changed the title with main and the color of the boxplot with col. The col argument can take values such as red,blue or any HEX code, see ?boxplot for further customization.\n\nboxplot(birth_weight,\n        main='Boxplot of Birth Weights',\n        xlab ='birth weights', ylab='ounces',\n        col='#61b1ed')\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor a collection of R colors by name refer to Rcolors. Another approach is to search ‘color picker’ in google and copy/paste the hex code\n\n\nWe can apply similar customization to our histogram\n\nhist(birth_weight,\n     main='Histogram of Birth Weights',\n     xlab ='birth weights',\n     col='#d1584f',\n     breaks=20)\n\n\n\n\nThe hist function uses the Sturges method by default to determine the number of breaks on the histogram. We can manually change the number of breaks, but we should be careful not to specify a low or high number of breaks. Usually the default setting is appropriate for most scenarios.\n\n\nCategorical variables\nOne way to extract all the character columns is using both Filter and is.character functions. A similar argument can be said for any columns that are factors, but instead we’d use is.factor.\nBelow are the first five rows of all the character columns in birth_dat\n\nFilter(is.character,birth_dat)\n\n\n\n#>   Gender Premie   Marital Racemom Racedad Hispmom Hispdad     Habit\n#> 1   Male     No   Married   White   White NotHisp NotHisp NonSmoker\n#> 2 Female     No Unmarried   White   White Mexican Mexican NonSmoker\n#> 3   Male     No Unmarried   White Unknown Mexican Unknown NonSmoker\n#> 4 Female     No Unmarried   White   White NotHisp NotHisp NonSmoker\n#> 5   Male     No   Married   White   Black NotHisp NotHisp NonSmoker\n#>   MomPriorCond BirthDef    DelivComp BirthComp\n#> 1         None     None At Least One      None\n#> 2         None     None At Least One      None\n#> 3 At Least One     None At Least One      None\n#> 4         None     None At Least One      None\n#> 5 At Least One     None         None      None\n\n\nThe names of the character columns can be obtained using colnames() function in combination with the above statement\n\ncolnames(Filter(is.character,birth_dat) )\n\n#>  [1] \"Gender\"       \"Premie\"       \"Marital\"      \"Racemom\"      \"Racedad\"     \n#>  [6] \"Hispmom\"      \"Hispdad\"      \"Habit\"        \"MomPriorCond\" \"BirthDef\"    \n#> [11] \"DelivComp\"    \"BirthComp\"\n\n\nWe will only consider the Hispmom variable from our dataset to demonstrate methods to summarize and visualize a character variable.\nFirst, we’ll save the values from Hispmom column into a separate variable and compute several categorical summaries\n\nhispanic_mom <- birth_dat$Hispmom\n\nThe table() function in R can be used to quickly create frequency tables.\n\ntable(hispanic_mom)\n\n#> hispanic_mom\n#>   Mexican   NotHisp OtherHisp \n#>       215      1693        84\n\n\nFrom the above frequency table we observe there were 25 mom who were Mexican, 1693 non Hispanic, and 84 were other types of Hispanic. We can easily convert the frequency table into a frequency table of proportions using prop.table(). The input for prop.table() is a table created using table().\n\nprop.table(table(hispanic_mom))\n\n#> hispanic_mom\n#>    Mexican    NotHisp  OtherHisp \n#> 0.10793173 0.84989960 0.04216867\n\n\nNow, we observe roughly 10.79% of moms were Mexican, 84.99% were non Hispanic and 4.22% were other types of Hispanic. Note that all of the proportions should add up to 1.\n\nsum(prop.table(table(hispanic_mom)))\n\n#> [1] 1\n\n\nWhile the above method works, it is not the only way to obtain frequency tables. We can obtain the same results using tally() from the mosaic library.\n\nmosaic::tally(hispanic_mom)\n\n#> X\n#>   Mexican   NotHisp OtherHisp \n#>       215      1693        84\n\n\nIf we want frequency tables of proportions, we need to use the argument format and specify format = 'proportion'. There are other formats such as 'count', 'percent' etc.. for more details run ?mosaicCore::tally()\n\nmosaic::tally(hispanic_mom,format='proportion')\n\n#> X\n#>    Mexican    NotHisp  OtherHisp \n#> 0.10793173 0.84989960 0.04216867\n\n\nTo plot a single categorical variable we can use barplot(). The input for barplot() when dealing with categorical data is a table, like the ones we created above\n\nbarplot(table(hispanic_mom))\n\n\n\n\nInstead of the frequency counts, we can plot frequency of proportions by inputting a frequency tables of proportions.\n\nbarplot(prop.table(table(hispanic_mom)),\n        main = 'Ethnicity Proportions of Moms',\n        col = '#d59cdb')"
  },
  {
    "objectID": "ucla/stats10/lab1.html#two-variables",
    "href": "ucla/stats10/lab1.html#two-variables",
    "title": "Jose Toledo Luna",
    "section": "Two variables",
    "text": "Two variables\n\nNumerical variables\nWe consider the following two numerical variables: Feduc and Meduc, which is the highest education for fathers and mothers in this dataset, respectively.\n\nfather_eduction <- birth_dat$Feduc\nmother_education <- birth_dat$Meduc\n\nWe can compare their distributions in a single plot as we did in Section 2 with boxplots\n\nboxplot(father_eduction,mother_education,\n        names = c('father','mother'),\n        col=c('#f5d376','#f5767c'),\n        main = 'Highest Education for Parernts',\n        ylab = 'Year')\n\n\n\n\nFor boxplots its pretty straight forward to compare two numerical distributions using the syntax boxplot(v1,v2,...). For histograms it requires a bit more work.\nWe start by creating a histogram for the first variable, then creating another histrogam for the second variable but using the argument add=TRUE. We must specify a unique color for each histogram representing the variables. In order for both of the histograms to fit properly on the same plot we must take into account the lowest and highest values among the multiple numerical variables.\n\nlow_x <- min(father_eduction,mother_education)\nhigh_x <- max(father_eduction,mother_education)\n\nLastly, we must specify a legend to appropriately distinguish the multiple histograms using the function legend().\n\nhist(father_eduction, col='lightcoral',\n     xlim=c(low_x,high_x),\n     main='Education for Parents', xlab='Year')\nhist(mother_education, col='lightblue', add=TRUE)\nlegend('topright', legend = c('father', 'mother'), \n       fill=c('lightcoral', 'lightblue'))\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using legend() it is important that you specify the correct ordering of colors for each group, otherwise the legend would be incorrect. For example, in our first histogram we chose “lightcoral” to represent “father” and “lightblue” to represent “mother” distributions. Which is why we used the arguments: legend=c('father','mother'), fill = c('lightcoral','lightblue') in that order\n\n\nWe can also consider a scatter plot to visualize the relationship between two numerical variables. We consider the two numerical variables Gained and weight. Gained describes the weight gained during the pregnancy term and weight describe the weight of the baby at birth.\n\nplot(x = birth_dat$Gained,y = birth_dat$weight,\n     main = 'Baby weight vs pregnancy weight gain',\n     xlab = 'weight gained during pregnancy',\n     ylab = 'Baby weight (oz.)',\n     col='lightcoral')\n\n\n\n\n\n\nCategorical variables\nFor this example, we consider the following two character variables Hispdad and Habit. Hispdad determines whether the father of the baby was Hispanic or not. In particular, are they Mexican, non-Hispanic, or other type of Hispanic ethnicity. Habit determines whether or not the subject had a smoking habit or not.\nWhen dealing with two categorical variables we can create a two-way table using table(v1,v2). Below is the table of frequency for both Habit and Hispdad.\n\ntable(birth_dat$Habit,birth_dat$Hispdad)\n\n#>            \n#>             Mexican NotHisp OtherHisp Unknown\n#>   NonSmoker     184    1236        78     307\n#>   Smoker          5     117         2      63\n\n\nFrom the above frequency table of counts you will notice that there were 184 Mexican dads who were non-smokers, 5 Mexican dads who were smokers, 1236 non-Hispanics who were non-smokers, 117 non-Hispanics who were smokers and similar interpretations can be made for the remaining cells.\nWe can obtain a frequency table of proportions using prop.table()\n\nprop.table(table(birth_dat$Habit,birth_dat$Hispdad))\n\n#>            \n#>                 Mexican     NotHisp   OtherHisp     Unknown\n#>   NonSmoker 0.092369478 0.620481928 0.039156627 0.154116466\n#>   Smoker    0.002510040 0.058734940 0.001004016 0.031626506\n\n\nYou will notice that summing each cell will give us 1\n\nsum(prop.table(table(birth_dat$Habit,birth_dat$Hispdad)))\n\n#> [1] 1"
  },
  {
    "objectID": "ucla/stats10/lab3.html",
    "href": "ucla/stats10/lab3.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "In this lab, you will\n\nUnderstand linear regression in R and verify linear regression assumptions\nUse R for sampling and simulation\n\nThe corresponding tutorials are listed below\n\nSimple Linear Regression\nSampling and Simulation"
  },
  {
    "objectID": "ucla/stats10/lab2.html",
    "href": "ucla/stats10/lab2.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "In this lab, you will"
  },
  {
    "objectID": "ucla/stats10/lab2.html#logical-statements",
    "href": "ucla/stats10/lab2.html#logical-statements",
    "title": "Jose Toledo Luna",
    "section": "Logical Statements",
    "text": "Logical Statements\nThere are only two logical values, TRUE and FALSE. In R, we can abbreviate TRUE with T and FALSE with F. They can be interpreted as any option corresponding to a binary choice. For example, yes/no, do/don’t, satisfied/not satisfied or even 1/0.\nA basic way to define a logical statement is using a relational operator to compare two expressions. For example, we may ask ourselves “is x less than a certain number ?” or using a real world example from the mtcars dataset “how many cars have more than 18 miles per gallon?”\n\nRelational operators\nThe table below summarizes some of the relational operators available in R:\n\n\n\nOperator\nInterpretation\nBasic Example\nResult\n\n\n\n\n==\nEqual to\n5 == 5\nTRUE\n\n\n!=\nNot equal to\n4 != 5\nTRUE\n\n\n>\nGreater than\n4 > 5\nFALSE\n\n\n<\nLess than\n4 < 5\nTRUE\n\n\n<=\nLess than or equal to\n4 <= 5\nTRUE\n\n\n>=\nGreater than or equal to\n4 >= 5\nFALSE\n\n\n\nFrom the table above we consider single numbers as our two expression to compare, but we can extend this idea to vectors, data.frames, matrices of various data types. When applying relational operators to vectors it is important to know they are being compared element-wise.\nWe first start off by comparing a vector with a single number\n\nc(1,3,5,7,9) < 5\n\n#> [1]  TRUE  TRUE FALSE FALSE FALSE\n\n\nInterpretation: Is 1 less than 5? is 3 less than 5? is 5 less than 5? is 7 less than 5? is 9 less than 5?\nThe output from the above example is a logical vector\n\nclass(c(1,3,5,7,9) < 5)\n\n#> [1] \"logical\"\n\n\nwith TRUE/FALSE if the given condition was satisfied or not. What if we were given the question “How many values of x are smaller than some number?”\n\nsum( c(1,3,5,7,9) < 5 )\n\n#> [1] 2\n\n\nwe can then apply the sum() function to count how many TRUE were in our logical vector. This will be very useful when we have very large vectors and we can’t count how many TRUE were in our vector manually.\nBelow are some examples applying relational operators to compare two vectors of the same length\n\nc(1,2,3,4) < c(5,4,3,2)\n\n#> [1]  TRUE  TRUE FALSE FALSE\n\n\nInterpretation: Is 1 less than 5? is 2 less than 4? is 3 less than 3? is 4 less than 2?\n\nc(1,2,3,4) <= c(5,4,3,2)\n\n#> [1]  TRUE  TRUE  TRUE FALSE\n\n\nInterpretation: Is 1 less than or equal to 5? is 2 less than or equal to 4? is 3 less than or equal to 3? is 4 less than or equal to 2?\nAnother topic to consider is comparing two strings. While this can be a more advance topic we only consider the simplest scenario in which we compare case-sensitive strings.\n\nstring1 <- 'Hello'\nstring2 <- 'hello'\n\nwhile the above strings contain the same characters in the same order, if we compare them directly they will be considered different\n\nstring1 == string2\n\n#> [1] FALSE\n\n\nInterpretation: are string1 and string2 identical?\nHowever, if were are interested in seeing if they contain the same characters regardless of the case sensitivity, we can use tolower() function as follows\n\ntolower(string1)\n\n#> [1] \"hello\"\n\ntolower(string2)\n\n#> [1] \"hello\"\n\n\ntolower() will convert any upper-case character in a vector into lower-case character.\n\ntolower(string1) == tolower(string2)\n\n#> [1] TRUE\n\n\nSince all the characters are now lower-case, and both strings contain the same characters in the same order then they are now identical.\nFor more advanced examples in comparing strings check out the following blog post (Optional)\n\n\nLogical operators\nIn practice, we often need to use multiple conditions to make certain decisions. For example, you have a personal rule that if there is no homework AND you don’t have class, then you will go out with your friends. Now, explore what happens to this rule when OR is used instead of AND, also what happens when negation (NOT ) is added to one or both clauses.\nThe table below summarizes some of these logical operators\n\n\n\n\n\n\n\n\n\nOperator\nInterpretation\nBasic Example\nResult\n\n\n\n\n!\nNOT  If the condition is true, logical NOT operator returns as false\n! (5 == 5)\nFALSE\n\n\n&\nAND (element-wise)  Returns true when both conditions are true\nTRUE & TRUE\nTRUE & FALSE\nFALSE & TRUE\nFALSE & FALSE\nTRUE\nFALSE\nFALSE\nFALSE\n\n\n&&\nAND (single comparison)  Same as above but for single comparison\n(same as & above)\n(same as & above)\n\n\n|\nOR (element-wise)  Returns true when at-least one of conditions is true\nTRUE |TRUE\nTRUE | FALSE\nFALSE | TRUE\nFALSE | FALSE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\n||\nOR  (single comparison)  Same as above but for single comparison\n(same as | above)\n(same as | above)\n\n\n\nThe difference between element-wise and single comparison can be seen in the examples below\n\nc(TRUE,TRUE,FALSE,FALSE) | c(TRUE,FALSE,TRUE,FALSE)\n\n#> [1]  TRUE  TRUE  TRUE FALSE\n\n\nInterpretation: TRUE or FALSE, TRUE or FALSE, FALSE or TRUE, FALSE or FALSE\nElement-wise will return a vector of logical values, one for each pair of logicals combined. Whereas, single comparison only compares the first two elements of the logical vectors and will return a single logical value.\n\nc(TRUE,TRUE,FALSE,FALSE) || c(TRUE,FALSE,TRUE,FALSE)\n\n#> Warning in c(TRUE, TRUE, FALSE, FALSE) || c(TRUE, FALSE, TRUE, FALSE):\n#> 'length(x) = 4 > 1' in coercion to 'logical(1)'\n\n\n#> [1] TRUE\n\n\nInterpretation: TRUE or TRUE\nThe above will output TRUE since it only considers the first elements of the respective vectors, but it will warn us that the length of the vectors have more than one element. A better example for single comparisons would be as follows\n\nage <- 20\nage > 10 && age < 30\n\n#> [1] TRUE\n\n\nInterpretation: Is age greater than AND less than 30?\n\nage == 18 || age <= 21\n\n#> [1] TRUE\n\n\nInterpretation: Is age 18 OR less than or equal to 21 ?\nConsider a more complicated example of holding office in the United States. The president must be a natural-born citizen of the United States, be at least 35 years old, and have been a resident of the United States for 14 years\n\ncandidate_age <- 40\ncandidate_birth <- 'United States'\ncandidate_residance_years <- 10\n\nWe have a candidate who is 40 years old, was born in the United States but for some reason they have only been a resident of the United States for 10 years. Clearly, this candidate is not eligible to become our next president. We demonstrate this using logical operators\n\ncandidate_age >= 35\n\n#> [1] TRUE\n\n\nInterpretation: Is the candidate at least 35 years old?\n\ncandidate_birth == 'United States'\n\n#> [1] TRUE\n\n\nInterpretation: Is the candidate born in United States?\n\ncandidate_residance_years >= 14\n\n#> [1] FALSE\n\n\nInterpretation: Has the candidate been a resident for at least 14 years?\nPutting all of the above together,\n\n(candidate_age >= 35) && (candidate_birth == 'United States') && (candidate_residance_years >= 14)\n\n#> [1] FALSE\n\n\nInterpretation: TRUE AND TRUE AND FALSE\nSince one of the conditions fails the entire statement will be false."
  },
  {
    "objectID": "ucla/stats10/lab2.html#subsetting",
    "href": "ucla/stats10/lab2.html#subsetting",
    "title": "Jose Toledo Luna",
    "section": "Subsetting",
    "text": "Subsetting\n\nVectors\nNow that we have an idea of how to construct logical statements, we can apply them to subset our data based on a given condition\nConsider the following vector dat with 18 values\n\ndat <- c(11, 13, 18, 3, 2, 24, 10, 8, 5, \n         13, 3, 23, 7, 25, 17, 20, 11, 17)\n\nWe will subset dat based on the following conditions:\n1. How many values are bigger than 10?\n\ndat > 10 \n\n#>  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n#> [13] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nsum(dat > 10 )\n\n#> [1] 11\n\n\nwhile knowing how many values are bigger than 10 is useful, we may only want to keep those values and not the ones that are smaller than 10.\n2. Keep the values that are bigger than 10?\nIf given a vector, the way to subset it based on a condition is as follows:  vector[ condtion ]. Our condition is all the values that are bigger than 10, that is dat > 10\n\ndat[ dat > 10 ]\n\n#>  [1] 11 13 18 24 13 23 25 17 20 11 17\n\n\n3. How many values are exactly 11 ?\nOur condition is dat == 11,this should only return two TRUE, and after using the sum() function to count them we obtain\n\nsum(dat == 11)\n\n#> [1] 2\n\n\nIf we wanted to extract these values from dat we would run\n\ndat[ dat == 11 ]\n\n#> [1] 11 11\n\n\nNext we use the birth dataset for the following examples\n4. How many females were in this dataset?\n\nbirth_dat <- read.csv(file = \"/Users/toledo60/Desktop/Projects/personal-website/ucla/stats10/data/births.csv\")\n\nFirst we extract the values from the Gender column and store them in a variable called gender_vec\n\ngender_vec <- birth_dat$Gender\n\n\nunique(gender_vec)\n\n#> [1] \"Male\"   \"Female\"\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRecall strings are case-sensitive, so you must spell ‘Female’ exactly as it appears above\n\n\nThen we subset this vector to only include females\n\nfemales_vec <- gender_vec[gender_vec == 'Female']\n\n\nunique(females_vec)\n\n#> [1] \"Female\"\n\n\nNow our vector only contains females, we can use length() to count how many females were in this dataset\n\nlength(females_vec)\n\n#> [1] 957\n\n\nAn easier approach would be to simply create the variable gender_vec and count how many females are in that vector\n\nsum(gender_vec == 'Female')\n\n#> [1] 957\n\n\n\n\nData frames\nConsidering example 4 in the vectors section of subsetting, we are extracting solely the values from a specific column based on a given condition. However, in some scenarios we may want to preserve all other information (columns) from our dataset after subsetting our data.\nData frames have the following structure data[rows,columns]. The first argument inside the brackets will specify the rows and the second argument will specify the columns. We can apply all of the subsetting techniques we covered in the vectors within the rows, columns, or both rows and columns data[condition for rows, condition for columns]\nFor example, if we wanted to subset the births dataset to only include females\n\nis_female <- birth_dat$Gender == 'Female'\n\n\nbirth_dat[is_female, ]\n\nInterpretation: Subset the rows to only include females, keep all the other columns\n\n\n#>   Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n#> 2 Female     No    177      8   36   26     9    12       2     11 Unmarried\n#> 4 Female     No    144      6   33   37    12    14       2     12 Unmarried\n#> 6 Female     No     98      4   31   29    14    16       3     20   Married\n#>   Racemom Racedad Hispmom Hispdad Gained     Habit MomPriorCond BirthDef\n#> 2   White   White Mexican Mexican     20 NonSmoker         None     None\n#> 4   White   White NotHisp NotHisp     50 NonSmoker         None     None\n#> 6   White   White NotHisp NotHisp     21 NonSmoker         None     None\n#>      DelivComp BirthComp\n#> 2 At Least One      None\n#> 4 At Least One      None\n#> 6         None      None\n\n\nYou will notice that we only applied a condition to the rows argument and not the columns argument. In the case where one of the arguments is left blank, then no condition will be applied to the respective argument.\nFor practice, consider the following examples\n1. Create a new data frame containing the columns: Gender, weight, and Habit\nWe can use colnames()\n\ncolnames(birth_dat)\n\n#>  [1] \"Gender\"       \"Premie\"       \"weight\"       \"Apgar1\"       \"Fage\"        \n#>  [6] \"Mage\"         \"Feduc\"        \"Meduc\"        \"TotPreg\"      \"Visits\"      \n#> [11] \"Marital\"      \"Racemom\"      \"Racedad\"      \"Hispmom\"      \"Hispdad\"     \n#> [16] \"Gained\"       \"Habit\"        \"MomPriorCond\" \"BirthDef\"     \"DelivComp\"   \n#> [21] \"BirthComp\"\n\n\nto make sure we have the correct spelling of the appropriate columns we want to keep.\n\nbirth2 <- birth_dat[ , c('Gender','weight','Habit')]\n\nInterpretation: Keep all the rows, but only keep the columns: Gender, weight, and Habit\n\nhead(birth2,3)\n\n#>   Gender weight     Habit\n#> 1   Male    124 NonSmoker\n#> 2 Female    177 NonSmoker\n#> 3   Male    107 NonSmoker\n\n\nWe created a character vector with the names of the columns we wanted to keep and used it as the condition in the columns argument.\n2. Split birth_dat into two parts: One for which the individual was a smoker and another for which they were not a smoker\nThe variable Habit contains information on whether or not the individual was a smoker.\n\nunique(birth_dat$Habit)\n\n#> [1] \"NonSmoker\" \"Smoker\"\n\n\nFirst we create a logical vector to determine if the individual was a smoker\n\nis_smoker <- birth_dat$Habit == 'Smoker'\n\n\nis_smoker[1:5]\n\n#> [1] FALSE FALSE FALSE FALSE FALSE\n\n\nInterpretation: Return TRUE if Habit is smoker, otherwise FALSE\nWe use the negation logical operator to obtain all the non-smokers from our logical vector is_smoker without having to create a new variable\n\n!is_smoker[1:5]\n\n#> [1] TRUE TRUE TRUE TRUE TRUE\n\n\nTo subset our data into keeping only the smokers we input our logical vector is_smoker into the rows argument\n\nsmokers <- birth_dat[is_smoker, ]\n\nInterpretation: Only keep the rows in which the individual is a smoker\n\nhead(smokers,3)\n\n#>    Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n#> 15 Female     No    106      8   28   29    13    12       3     15   Married\n#> 17 Female     No    115      8   30   18    12    10       1     20 Unmarried\n#> 18   Male     No    128      9   21   19    12    12       2     13 Unmarried\n#>    Racemom Racedad Hispmom Hispdad Gained  Habit MomPriorCond BirthDef\n#> 15   White   White NotHisp NotHisp     37 Smoker         None     None\n#> 17   White Unknown NotHisp Unknown     45 Smoker         None     None\n#> 18   White   Black NotHisp NotHisp     35 Smoker         None     None\n#>       DelivComp BirthComp\n#> 15         None      None\n#> 17         None      None\n#> 18 At Least One      None\n\n\nTo subset our data into keeping only the non-smokers we input our logical vector !is_smoker into the rows argument\n\nnot_smokers <- birth_dat[!is_smoker, ]\n\nInterpretation: Only keep the rows in which the individual is NOT a smoker\n\nhead(not_smokers,3)\n\n#>   Gender Premie weight Apgar1 Fage Mage Feduc Meduc TotPreg Visits   Marital\n#> 1   Male     No    124      8   31   25    13    14       1     13   Married\n#> 2 Female     No    177      8   36   26     9    12       2     11 Unmarried\n#> 3   Male     No    107      3   30   16    12     8       2     10 Unmarried\n#>   Racemom Racedad Hispmom Hispdad Gained     Habit MomPriorCond BirthDef\n#> 1   White   White NotHisp NotHisp     40 NonSmoker         None     None\n#> 2   White   White Mexican Mexican     20 NonSmoker         None     None\n#> 3   White Unknown Mexican Unknown     70 NonSmoker At Least One     None\n#>      DelivComp BirthComp\n#> 1 At Least One      None\n#> 2 At Least One      None\n#> 3 At Least One      None\n\n\n3. What is the average weight of babies with at least one birth defect?\nThe variable BirthDef determines if the baby had no birth defects or had at least one defect\n\nunique(birth_dat$BirthDef)\n\n#> [1] \"None\"         \"At Least One\"\n\n\nCreate a logical vector to determine if the baby had at least one defect\n\nhas_defect <- (birth_dat$BirthDef == 'At Least One')\n\n\n\n\n\n\n\nNote\n\n\n\nWe must spell “At Least One” with correct upper/lower cases including spaces\n\n\n\nhas_defect[1:5]\n\n#> [1] FALSE FALSE FALSE FALSE FALSE\n\n\nSubset our data to include rows with babies with at least one defect, then select only the weight column. Lastly compute the mean.\n\nmean( birth_dat[has_defect,'weight'] )\n\n#> [1] 115.8\n\n\nInterpretation: Average weight of babies with at least one birth defect"
  },
  {
    "objectID": "ucla/stats10/lab4.html",
    "href": "ucla/stats10/lab4.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "In this lab you will:\n\nReinforce understanding of simulating and random sampling\nDemonstrate the Central Limit Theorem’s application to means\n\nThe corresponding tutorials are listed below\n\nNormal Distribution\nCLT"
  },
  {
    "objectID": "ucla/stats10/normal_distribution.html",
    "href": "ucla/stats10/normal_distribution.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "Packages required for this tutorial"
  },
  {
    "objectID": "ucla/stats10/normal_distribution.html#normal-distribution",
    "href": "ucla/stats10/normal_distribution.html#normal-distribution",
    "title": "Jose Toledo Luna",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nOne of the most important and widely used continuous distribution is the Normal distribution, or Gaussian distribution.\nLet \\(X \\sim N(\\mu,\\sigma)\\) be a random variable following a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). In R, the following functions described in the table below, allows us to summarize the function relating to the normal distribution\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ndnorm\nNormal density\n(Probability Density Function)\n\n\npnorm\nNormal distribution\n(Cumulative Distribution Function)\n\n\nqnorm\nQuantile function of the Normal distribution\n\n\nrnorm\nNormal random number generation\n\n\n\nBy default, all of the functions above consider the standard Normal distribution, which has a mean of zero and a standard deviation of one, \\(X \\sim N(0,1)\\)\n\ndnorm\nThe density function for a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp{\\left(\n-\\frac{1}{2\\sigma^2 }(x-\\mu)^2\n\\right)}\n\\] for \\(-\\infty < x < \\infty\\)\nWe can use dnorm() function to calculate the density function, i.e \\(f(x)\\), for a grid of \\(x\\) values from any normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)\nFor example, we can calculate \\(f(0)\\) from a standard normal distribution\n\ndnorm(x = 0,mean = 0,sd = 1)\n\nConsider evaluating \\(f(x)\\) for \\(x \\in [1,10]\\) with mean 1 and standard deviation of 3\n\ndnorm(x=1:10, mean = 1, sd =3 )\n\n\n\npnorm\nThe pnorm() function gives the Cumulative Distribution Function (CDF) of the Normal distribution, which is the probability that the variable \\(X\\) takes a value less than or equal to \\(x\\). Mathematically, \\(F_X(x) = P(X \\leq x)\\).\nFor any continuous distribution \\(P(X = x)=0\\), so equivalently the CDF is \\(P(X \\leq x) = P(X < x)\\).\nConsider the standard normal distribution, since this distribution is symmetrical centered around \\(\\mu=0\\) then \\(P(X \\leq 0) = 0.5\\). We can verify this result using pnorm as follows\n\npnorm(0,mean = 0, sd = 1)\n\nExample: Suppose \\(X\\) is the SAT-M score which has a normal distribution with a mean of 507 and standard deviation of 111. What is the probability of scoring less than 700 on the SAT-M?\n\n\nShow Code\n\n\nprob1 <- round(pnorm(700, mean=507, sd=111) * 100,2)\np1 <- ggplot(data.frame(x = c(150,900)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(150,900),\n                args = list(\n                  mean = 507,\n                  sd = 111\n                )) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(150, 700),\n                args = list(\n                  mean = 507,\n                  sd = 111\n                ))+\n  annotate(\"text\", x = 510, y = 0.0015, \n           label = paste0(prob1,'%'),\n           size = 8)+\n  geom_vline(xintercept = 700,linetype =2)+\n  labs(x = '',y= 'Density')\n\n\n\n\n\nThat is \\(P(X < 700)\\),\n\npnorm(700, mean=507, sd=111)\n\nWhat about the probability of scoring greater than 700?\n\n\nShow Code\n\n\nprob2 <- round(pnorm(700, mean=507, sd=111,\n                     lower.tail = FALSE)*100,2)\n\np2 <- ggplot(data.frame(x = c(150,900)), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = c(150,900),\n                args = list(\n                  mean = 507,\n                  sd = 111\n                )) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(700, 900),\n                args = list(\n                  mean = 507,\n                  sd = 111\n                ))+\n  annotate(\"text\", x = 738, y = 0.00017, \n           label = paste0(prob2,'%'),\n           size=4.7)+\n  geom_vline(xintercept = 700,linetype =2)+\n  labs(x = '',y= 'Density')\n\n\n\n\n\nWe are interested \\(P(X > 700)\\), which can be obtained through \\(P(X > 700) = 1- P(X \\leq 700)\\)\n\n1-pnorm(700, mean = 507, sd = 111)\n\nAlternative, pnorm() has an argument lower.tail=TRUE (by default). If lower.tail=TRUE, the probabilities \\(P(X \\leq x)\\) are returned. Otherwise, if lower.tail=FALSE, \\(P(X > x)\\) are returned\n\npnorm(700, mean = 507, sd = 111, lower.tail = FALSE)\n\nThe Empirical rule (also known as the 68-95-99.7 rule) is a statistical rule stating that for a normal distribution, where most of the data will fall within three standard deviations of the mean. The empirical rule can be broken down into three parts: 68% of data falls within the first standard deviation from the mean (blue shaded region). 95% fall within the 2nd standard deviations (up to the green shaded region). 99.7% fall within third standard deviation (up to the red shaded region)\n\n\nShow Code\n\n\nx_limits <- c(-4,4)\np1 <- ggplot(data.frame(x = x_limits), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = x_limits) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(-1, 1))+\n  labs(x = '',y= 'Density')+\n  annotate(\"text\", x = 0.1, y = 0.15, \n           label = paste0(68,'%'),\n           size=4.5)+\n  scale_x_continuous(name = '',limits = x_limits,\n                     breaks = -4:4)\n\n\np2 <- ggplot(data.frame(x = x_limits), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = x_limits) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = \"green\",\n                alpha =0.3,\n                xlim = c(-2, 2)) +\n  labs(x = '',y= '')+\n  annotate(\"text\", x = 0, y = 0.15, \n           label = paste0(95,'%'),\n           size=4.5)+\n  scale_x_continuous(name = '',limits = x_limits,\n                     breaks = -4:4)\n\n\np3 <- ggplot(data.frame(x = x_limits), aes(x)) +  \n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = x_limits) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = \"red\",\n                alpha = 0.3,\n                xlim = c(-3, 3))+\n  labs(x='',y='',)+\n  annotate(\"text\", x = 0.1, y = 0.15, \n           label = paste0(99.7,'%'),\n           size=4.5)+\n  scale_x_continuous(name = '',limits = x_limits,\n                     breaks = -4:4)\n\n\nplots <- p1+p2+p3+plot_layout(ncol = 3)+\n  plot_annotation(title = '69-95-98.7 Rule')\n\n\n\n\n\nWe can easily verify these results using pnorm. Assuming a standard Normal distribution if we were one stanard deviation away from the mean then\n\npnorm(1)-pnorm(-1)\n\nIf we were two standard devations away from the mean\n\npnorm(2) - pnorm(-2)\n\nand lastly, three standard deviations away from the mean\n\npnorm(3) - pnorm(-3)\n\n\n\nqnorm\nThe function qnorm() returns the value of the inverse cumulative density function (CDF) of the normal distribution with specified mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nLet \\(F_X(x) = P(X \\leq x)\\) be the CDF of the normal distribution, and suppose it returns the probability \\(p\\), i.e, \\(F_X(x) = p\\). The inverse of the CDF or (quantile function) tells you what \\(x\\) would make \\(F_X(x)\\) return some probability \\(p\\);\n\\[F_X^{-1}(p) = x\\] For example, for the standard normal distribution \\(F_X(0)=P(X \\leq 0) = 0.5\\). That is, the value of \\(x\\) or (quantile) which gives us a cumulative probability of 0.5 is \\(x=0\\).\nTherefore, we can use the qnorm() function to find out what value of \\(x\\) or (quantile) gives us a a cumulative probability of \\(p\\). Hence, the qnorm function is the inverse of the pnorm function\n\n\nShow Code\n\n\nx_limits <- c(-4,4)\n\np1 <- ggplot(data.frame(x = x_limits), aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\",\n                xlim = x_limits) +\n  stat_function(fun = dnorm,\n                geom = \"area\",\n                fill = 'steelblue',\n                alpha =0.3,\n                xlim = c(-4, 0))+\n  geom_vline(xintercept = 0,linetype=2,color = '#cf5d55')+\n  labs(x = '',y= 'Density')+\n  annotate('text', x = -0.65, y = 0.17, \n           parse =TRUE,\n           label = paste0(\n             expression('F'[x]),'(0)==0.5') )+\n  annotate('text',x = 0.77, y = 0.41,\n           parse =TRUE,\n           label = paste0(\n             expression('F'[x]^{-1}),' *(0.5)==0'),\n           color = '#cf5d55')+\n  scale_x_continuous(name = '',limits = x_limits,\n                     breaks = -4:4)\n\n\n\n\n\n\nqnorm(p=0.5)\n\nGoing back to our example of the SAT-M scores. Suppose \\(X\\) is the SAT-M score which has a normal distribution with a mean of 507 and standard deviation of 111. Recall the probability of obtaining a score less than 700 on the SAT-M was \\(P(X < 700) = 0.9589596\\). Therefore, if we were interesting in finidng the score or (quantile) which gives us a cumulative probability of roughly 96% we can use qnorm as follows:\n\nqnorm(0.9589596, mean=507, sd=111)\n\nWe should see that the output value is exactly 700.\n\n\nrnorm\nThe rnorm function generates \\(n\\) observations from a Normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nWe specify a seed for reproducibility,\n\nset.seed(10)\n\nLet’s start by generate 10 random observations from a standard normal distribution\n\nrnorm(10, mean = 0, sd = 1)\n\nor equivalently,\n\nrnorm(10)\n\nWe can specify a different mean and standard deviation\n\nrnorm(10, mean = 10, sd = 2)\n\nIn the following plot we generate \\(n=100,1000,10000\\) random observations from a standard normal distribution. If we increase the number of observations, we see the data will approach the true Normal density function\n\n\nShow Code\n\n\np100 <- ggplot(data = data.frame(x=rnorm(100)),\n               aes(x))+\n  geom_histogram(aes(y = after_stat(density)),\n                 colour = 1, fill = \"steelblue\",\n                 alpha =0.3,\n                 bins=30) +\n  geom_density(linewidth = 1.2,\n               linetype = 2,\n               colour = 2)+\n  labs(x= '',y = 'density')+\n  facet_grid(~100)\n\np1000 <- ggplot(data = data.frame(x=rnorm(100)),\n                aes(x))+\n  geom_histogram(aes(y = after_stat(density)),\n                 colour = 1, fill = \"steelblue\",\n                 alpha = 0.3,bins=30) +\n  geom_density(linewidth = 1.2,\n               linetype = 2,\n               colour = 2)+\n  labs(x= '',y = '')+\n  facet_grid(~1000)\n\np10000 <- ggplot(data = data.frame(x=rnorm(10000)),\n                 aes(x))+\n  geom_histogram(aes(y = after_stat(density)),\n                 colour = 1, fill = \"steelblue\",\n                 alpha = 0.3,bins=30) +\n  geom_density(linewidth = 1.2,\n               linetype = 2,\n               colour = 2)+\n  labs(x= '',y = '')+\n  facet_grid(~10000)\n\nplots <- p100 + p1000 + p10000 + plot_layout(ncol = 3)"
  },
  {
    "objectID": "ucla/stats10/transformations.html",
    "href": "ucla/stats10/transformations.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "In this lab, you will learn\nSuppose you have a response variable \\(y\\) which needs to be transformed, say for variance stabilization purposes. Some basic transformations are listed below\nDepending on the application some transformations may be more appropriate than others.\nFirst, lets generate a random response variable \\(y\\) with 50 values. We specify a seed using set.seed() in order to reproduce similar results shown below"
  },
  {
    "objectID": "ucla/stats10/transformations.html#log-transformation",
    "href": "ucla/stats10/transformations.html#log-transformation",
    "title": "Jose Toledo Luna",
    "section": "Log transformation",
    "text": "Log transformation\nIf we apply the log transformation to our response variable we simply calculate \\(log(y)\\)\n\nlog_y <- log(y)\nlog_y\n\n#>  [1] -1.2555265 -0.7845697 -0.6868220 -2.0170582 -1.0933399 -2.8449680\n#>  [7] -1.4517649 -1.8971057 -1.9954102 -1.1504930 -2.6544457 -3.1203434\n#> [13] -1.0786607 -2.0003929 -0.5447842 -0.4090738 -1.3776515 -2.3368539\n#> [19] -1.1921113 -1.4162132 -3.3619262 -1.7551837 -1.7184493 -1.4126140\n#> [25] -1.7147510 -1.2866305 -2.1356067 -1.5155314 -1.3147667 -3.4518601\n#> [31] -1.4154172 -1.5669097 -2.3458921 -1.5275952 -2.4033084 -2.5134832\n#> [37] -2.8139514 -1.1373870 -1.5220103 -1.5587653 -1.4488723 -1.7996969\n#> [43] -1.5205746 -2.3088713 -1.3722589 -1.9525983 -1.5399610 -2.3176438\n#> [49] -0.3066347 -2.3775205\n\n\nUsing histograms we can visualize the distributions to see the differences in our response variable after applying a log-transformation\n\nhist(y,main='Original',\n     xlab = 'y',\n     col = '#f56356')\nhist(log_y,main='Log-Transformation',\n     xlab = 'log(y)',\n     col = '#568bf5')\n\n\n\n\n\n\n\n\n\n\n\nAfter applying the log-transformation you’ll notice that it is more normally distributed (bell-shaped curve) compared to the original distribution.\nIt is important to take into account the domain for which the transformation is valid. For the log-transformation we need all values from our response variable to be greater than zero, otherwise we will obtain the following errors\n\nlog(0)\n\n#> [1] -Inf\n\nlog(-1)\n\n#> Warning in log(-1): NaNs produced\n\n\n#> [1] NaN"
  },
  {
    "objectID": "ucla/stats10/transformations.html#square-root-transformation",
    "href": "ucla/stats10/transformations.html#square-root-transformation",
    "title": "Jose Toledo Luna",
    "section": "Square root transformation",
    "text": "Square root transformation\nAnother transformation to consider is the square root transformation. This can be done using sqrt()\n\nsqrt_y <- sqrt(y)\n\n\nhist(y,main='Original',\n     xlab = 'y',\n     col = '#f56356')\nhist(sqrt_y,main='Square Root-Transformation',\n     xlab = 'sqrt(y)',\n     col = '#568bf5')"
  },
  {
    "objectID": "ucla/stats10/transformations.html#square-transformation",
    "href": "ucla/stats10/transformations.html#square-transformation",
    "title": "Jose Toledo Luna",
    "section": "Square transformation",
    "text": "Square transformation\nThe last transformation we consider is the square transformation. This can be done using y^2. In general any power transformation can be applied in a similar manner, \\(y^{(a)}\\) where \\(a\\) is any real number\n\ny_squared <- y^2\n\n\nhist(y,main='Original',\n     xlab = 'y',\n     col = '#f56356')\nhist(y_squared,main='Square Transformation',\n     xlab = 'y^2',\n     col = '#568bf5')\n\n\n\n\n\n\n\n\n\n\n\nSince we are squaring small numbers the resulting distribution will be more skewed right than the original distribution"
  },
  {
    "objectID": "ucla/stats10/sampling_simulation.html",
    "href": "ucla/stats10/sampling_simulation.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "We can obtain a random sample from existing elements in a dataset, vector, or list using the sample() function in R\nThe basic syntax for the sample() function is as follows:\nsample(x,size,replace = FALSE, prob = NULL)\n\nx: object we are sampling from, most often a vector or dataset\nsize: size of the sample\nreplace: should sampling be with replacement? (FALSE by default)\nprob: A vector of probability weights for obtaining the elements of the vector being sampled (NULL by default, every element is equally likely to be drawn)\n\nFor more details on the sample function run ?sample in the console\nIn order to replicate the following examples and get similar results, we can use set.seed()\n\nset.seed(123)\n\nSuppose we have a vector with 10 elements in it\n\nvec <- 1:10\n\nTo generate a random sample of five elements from our vector vec, we run the following command\n\nsample(vec,size = 5)\n\nRecall, the default setting for sampling is without replacement. This means once we draw the number (or element) from our vector we can not draw it again, similar to the lottery method.\nIt is also important to note that each time we run the command sample() it may generate a different set of elements each time\n\nsample(vec,size = 5)\n\nWe can also sample random elements from our vector using the argument replace=TRUE so that we are sampling with replacement. That is, each time we draw an element we put it back into our vector and there may be a chance we draw it again. So each element in the vector can be chosen to be in the random sample more than once\n\nsample(vec,size=5,replace = TRUE)\n\nsample() can be applied not only to numerical vectors, but other types of vectors such as character or logical.\nFor example, consider the character vector with the following names from our class. No one is volunteering so I randomly choose a set of three people in the class\n\nstudents <- c('Leslie', 'Ron', 'Andy', 'April', 'Tom', 'Ben', 'Jerry')\n\n\nsample(students, 3)\n\nIn this scenario sampling with replacement (i.e replace = TRUE) would not be appropriate\nOne important application for sampling is splitting up our dataset into two sets: one for training and one for testing an algorithm.\nWe will consider the penguins dataset from the palmerpenguins library\n\ndat <- palmerpenguins::penguins\n\n\nnrow(dat)\n\nIn order to sample random rows from a dataset you first have to create a vector from 1:nrow(data), these will be the indices we will sample from\n\nrow_indices <- 1:nrow(dat)\n\nOur training set will consist of 80% of the dataset and testing set will consist of the remaining 20%\n\ntrain_sampled_rows <- sample(row_indices, size= 0.8*nrow(dat) )\n\nInterpretation: randomly select 80% (approx. 275) rows from 1:344\n\ntrain_sampled_rows[1:10]\n\n\ntraining_set <- dat[train_sampled_rows,]\ndim(training_set)\n\nlastly, we use the remaining rows for our testing set\n\ntest_set <- dat[-train_sampled_rows, ]\ndim(test_set)"
  },
  {
    "objectID": "ucla/stats10/sampling_simulation.html#simulation",
    "href": "ucla/stats10/sampling_simulation.html#simulation",
    "title": "Jose Toledo Luna",
    "section": "Simulation",
    "text": "Simulation\nLet start with a simple example by simulating a fair dice roll\n\nsample(1:6,size=1)\n\nsince we are only drawing once, it doesn’t matter if you sample with or without replacement\nNow, consider rolling two fair, six-sided dice and computing their sum. One approach to compute this would be\n\ndice1 <- sample(1:6, size=1)\ndice2 <- sample(1:6, size=1)\ndice1 + dice2\n\nAn easier way:\n\nroll_two_dice <- sample(1:6,size = 2,replace = TRUE)\n\n\nsum(roll_two_dice)\n\nWe are drawing two numbers from the range 1 through 6 with replacement. We sample with replacement because the roll of each dice is independent and it’s possible to roll (or draw) the same number twice. If we were to use replace=FALSE once we draw, say 1, we could not draw 1 again for the second dice.\nFrom the above examples, it is straightforward to carry out a simple random experiment. What if we wanted to repeat these experiment multiple times? For example, we wanted to repeat the experiment of drawing two dice and calculating their sum 100 or even 1000 times. For such scenarios we can utilize the replicate() function in R.\nreplicate() implements common tasks involving for loops without explicitly having to use looping syntax.\nThe basic syntax for the replicate() function is as follows:\nreplicate(n,expr, simplify = 'array')\n\nn: number of replications. How many times do you want to replicate the experiment\nexpr: the expression to evaluate repeatedly\nsimplify: how the output should be returned. The default is an array. If simplify=FALSE, the output will be a list of n elements\n\nLet’s start by simulating sampling 3 different numbers from ranges 1 to 20 at random without replacement, 10 times.\nBreaking it down, first we create our expression (or experiment)\n\nset.seed(10)\nsample(1:20, size=3, replace=FALSE)\n\nthen replicate this experiment 10 times\n\nreplicate(n = 10, \n          expr = sample(1:20, size=3, replace=FALSE))\n\nThe default output of replicate() will be an array with n columns, the rows will depend on the length of the output from the experiment. For example, we are sampling 3 numbers from 1-20 so we will have three rows, each column will then correspond to a replicate of the experiment. So in the first replicate we drew the numbers (16,12,8). In the second replicate we drew the numbers (7,19,15), and similar interpretations hold for the remaining columns (replicates)\nGoing back to our example of calculating the sum of rolling two fair dice. We will replicate this experiment n times\nWe will use replicate as follows: 1. Write a function that performs the experiment once 2. Replicate the experiment using replicate() many times\nCreating a function\nWe can create a function that will calculate the sum of rolling two fair dice as follows\n\nsum_dice_roll <- function(){\n  dice_roll <- sample(1:6, size=2,replace = TRUE)\n  return(sum(dice_roll))\n}\n\nThe sum of rolling two fair dice once was\n\nsum_dice_roll()\n\nReplicate experiment multiple times\nLet’s start by calculating the sum of rolling two fair dice 20 times\n\nreplicate(n = 20,\n          expr = sum_dice_roll() )\n\nreplicating our experiment 100 times\n\nrep100 <- replicate(n = 100,\n                    expr = sum_dice_roll())\n\nIf we were to repeat this experiment many times, what is the sum that will most likely occur in the long run ?\nLooking at our experiment replicated 100 times we obtain the following relative frequency of every outcome. Note the possible outcomes are (2,3,4,…12)\n\nprop.table( table(rep100) ) \n\n\nbarplot( prop.table(table(rep100)),\n         xlab = 'Sum', ylab = 'Relative Frequency')\n\nReplicating our experiments many more times, say 10,000 times, we will obtain more stable results which comes from the idea of “long-run” behavior. According to the law of large numbers, if we repeat an experiment independently a large number of times and average the result, we should obtain a value which is close to the actual expected value.\n\nrep10000 <- replicate(n = 10000,\n                    expr = sum_dice_roll())\n\n\nprop.table( table(rep10000) ) \n\n\nbarplot( prop.table(table(rep10000)),\n         xlab = 'Sum', ylab = 'Relative Frequency')\n\nDepending on the context of the problem 10,000 may not be considered large, but for the given dice roll experiment 10,000 is enough. The more replicates you perform the longer time it will take to run on your computer."
  },
  {
    "objectID": "ucla/stats10/CLT.html",
    "href": "ucla/stats10/CLT.html",
    "title": "Jose Toledo Luna",
    "section": "",
    "text": "The sampling distribution of the sample mean is the theoretical distribution of means that would result from taking all possible samples of size \\(n\\) from the population.\nSuppose we are sampling from a population which comes from a standard Normal distribution such that observations in our sample are iid (independent, identically distributed). Each sample of size \\(n\\) will then consists of realizations \\(\\{x_1,\\dots,x_n\\}\\), such that each \\(x_i\\) will be a realization of the random variable \\(X_i \\sim N(0,1)\\)\nTo obtain the sampling distribution for the sample mean,\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population\nCompute the sample mean and store the result in a variable, say sample_means\nPlot each of the sample_means using a histogram\n\n\nn_samples <- 10000\nsample_size <- 100 # each sample will be of size 100\nsample_means <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = sample_size, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist <- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '')\n\n\n\n\n\nThe plot above is the distribution of sample means after taking 10,000 samples with size \\(n=100\\) from the population. The overall distribution appears to be normally distributed (bell shaped curve).\nThe Central Limit Theorem (CLT) states that if you have a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) and take sufficiently large random samples from the population with replacement, then the distribution of the sample means (sampling distribution) will be approximately normally distributed with mean \\(\\mu\\) and standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\). Note, the standard deviation of the sampling distribution of the sample mean (or any other statistic) is referred to as the standard error.\nRecall, the population is distributed as a standard normal distribution, i.e \\(N(\\mu=0,\\sigma=1)\\)\n\nmean(sample_means)\n\nwhich is very close to the population mean of \\(\\mu =0\\)\nwith standard error\n\nsd(sample_means)\n\nwhich is approximately \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{1}{10}\\)\nBelow is a demonstration of how increasing the sample size affects the distribution of the sample mean and seeing CLT in action\n\n\nShow Code\n\n\nn_samples <- 100\nsample_means <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = n_samples, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\nsample_mean <- round(mean(sample_means),3)\nstandard_error <- round(sd(sample_means),3)\n\nsampling_dist100 <- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '',\n       subtitle = paste0('mean=',sample_mean,\n                         ', standard error=',standard_error))+\n  facet_grid(~100)\n\n\nn_samples <- 1000\nsample_means <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = n_samples, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\nsample_mean <- round(mean(sample_means),3)\nstandard_error <- round(sd(sample_means),3)\n\nsampling_dist1000 <- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '',\n       subtitle = paste0('mean=',sample_mean,\n                         ', standard error=',standard_error))+\n  facet_grid(~1000)\n\n\nn_samples <- 10000\nsample_means <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = n_samples, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\nsample_mean <- round(mean(sample_means),3)\nstandard_error <- round(sd(sample_means),3)\n\nsampling_dist10000 <- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '',\n       subtitle = paste0('mean=',sample_mean,\n                         ', standard error=',standard_error))+\n  facet_grid(~10000)\n\n\nn_samples <- 20000\nsample_means <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rnorm(n = n_samples, mean = 0, sd=1) # generate a new sample\n  sample_means[i] = mean(sample_i) # obtain mean for each sample\n}\n\nsample_mean <- round(mean(sample_means),3)\nstandard_error <- round(sd(sample_means),3)\n\nsampling_dist20000 <- ggplot(data.frame(sample_means),aes(sample_means))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample means',y = '',\n       subtitle = paste0('mean=',sample_mean,\n                         ', standard error=',standard_error))+\n  facet_grid(~20000)\n\n\nplots <- sampling_dist100 + sampling_dist1000 + sampling_dist10000 +\n  sampling_dist20000 + plot_layout(nrow=2, ncol=2)\n\n\n\n\n\nTo interactively visualize how the sampling distribution of the sample mean builds up one sample at a time and see when the Central Limit Theorem start to kick in, I highly recommend the art of stats sampling distributions and Central Limit Theorem web applications.\n\n\n\nThe idea behind generating a sampling distribution for sample proportions is very similar to the procedure described in Sample means section.\nTo obtain the sampling distribution for the sample proportions;\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population\nCompute the sample proportion and store the result in a variable, say sample_proportions\nPlot each of the sample_proportions using a histogram\n\nHere the sample proportion is the fraction of samples which were success. We will take a large number of random samples from the population, where the population is generated from a binomial distribution with 10 trials and probability of success \\(p=0.25\\) for each trial.\n\nn_samples <- 10000\nsample_size <- 100 # each sample will be of size 100\nsample_proportions <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  rbinom(n = sample_size, size= 10, \n                     prob = 0.25)/sample_size # generate a new sample\n  sample_proportions[i] = mean(sample_i) # obtain proportion for each sample\n}\n\n\n\nShow Code\n\n\nsampling_dist_prop <- ggplot(data.frame(sample_proportions),\n                             aes(sample_proportions))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution',\n       x = 'Sample proportions',y = '')\n\n\n\n\n\nThe plot above is the distribution of sample proportions after taking 10,000 samples with size \\(n=100\\) from the population. The overall distribution appears to be normally distributed (bell shaped curve).\nThrough the CLT, the sampling distribution for the sample proportions will be distributed with mean \\(p\\) and standard error \\(\\sqrt{\\frac{p(1-p)}{n}}\\), \\(\\hat{p} \\sim N(p,\\sqrt{\\frac{p(1-p)}{n}} )\\)\n\nmean(sample_proportions)\n\nwhich is approximately the population parameter \\(p=0.25\\).\nThe standard error is\n\nsd(sample_proportions)"
  },
  {
    "objectID": "ucla/stats10/CLT.html#applications",
    "href": "ucla/stats10/CLT.html#applications",
    "title": "Jose Toledo Luna",
    "section": "Applications",
    "text": "Applications\nNow we will apply the the concepts of generating a sampling distribution for the sample mean using a real world dataset. In particular, we will use the penguins dataset from the palmerpenguins package.\n\nSample mean\nUsing the penguins dataset we will generate a sampling distribution for the sample mean of the penguins’ body mass (grams)\nFor simplicity, we will remove any missing observations\n\nbody_mass_g <- penguins$body_mass_g[!is.na(penguins$body_mass_g)]\n\nRecall, to obtain the sampling distribution for the sample mean, do the following process\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population, penguins$body_mass_g\nCompute the sample mean of the penguins body mass and store the result in a variable, say body_mass_xbar\nPlot each of the body_mass_xbar using a histogram\n\n\n\nShow Code\n\n\nn_samples <- 10000\nsample_size <- 50\n\nbody_mass_xbar <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(body_mass_g, size = sample_size) # generate a new sample\n  body_mass_xbar[i] = mean(sample_i) # obtain mean for each sample\n}\n\nsampling_dist <- ggplot(data.frame(body_mass_xbar),\n                        aes(body_mass_xbar))+\n  geom_histogram(fill = 'steelblue',alpha = 0.3,\n                 color = 'black',bins=30)+\n  labs(title = 'Sampling distribution of Sample Mean',\n       subtitle = 'Penguins body mass (grams)',\n       x = 'Sample means',y = '')\n\n\n\n\n\nThe mean for the sampling distribution is\n\nmean(body_mass_xbar)\n\nwhich is approximately the population mean for the penguins body mass (without missing observations)\n\nmean(body_mass_g)\n\nthe standard error for the sampling distribution is\n\nsd(body_mass_xbar)\n\n\n\nSample proportion\nUsing the penguins dataset we will generate a sampling distribution for the sample proportion of the Adelie penguins\nFor simplicity, we will remove any missing observations\n\nspecies <- penguins$species[!is.na(penguins$species)]\n\nRecall, to obtain the sampling distribution for the sample proportion\nFor the total number of samples do the following:\n\nGenerate a random sample of size \\(n\\) from the population, penguins$species (without missing observations)\nCompute the sample proportion of the penguins Adelie species and store the result in a variable, say adelie_proportions\nPlot each of the adelie_proportions using a histogram\n\n\n\nShow Code\n\n\nn_samples <- 10000\nsample_size <- 50\n\nadelie_proportions <- numeric(n_samples)\n\nfor(i in 1:n_samples){\n  sample_i =  sample(species, size = sample_size) # generate a new sample from the population\n  adelie_proportions[i] = mean(sample_i == 'Adelie') # obtain mean for each sample\n}\n\nsampling_dist <- ggplot(data.frame(adelie_proportions),\n                        aes(adelie_proportions))+\n  geom_bar(fill = 'steelblue',alpha = 0.3,\n                 color = 'black')+\n  labs(title = 'Sampling distribution of Sample Proportion',\n       subtitle = \"Adelie penguins\",\n       x = 'Sample proportions',y = '')\n\n\n\n\n\nThe mean for the sampling distribution is\n\nmean(adelie_proportions)\n\nwhich is approximately the population proportion for the Adelie penguins (without missing observations)\n\nprop.table(table(species))['Adelie']\n\nthe standard error for the sampling distribution is\n\nsd(adelie_proportions)\n\nwhich should approximately be \\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\np <- prop.table(table(species))['Adelie']\nsqrt((p*(1-p))/sample_size)"
  }
]